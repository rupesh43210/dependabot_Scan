#!/usr/bin/env python3
"""
Vulnerability Scanner - Optimized Version

A professional vulnerability scanner for GitHub Enterprise repositories
that extracts Dependabot security alerts with intelligent CVSS scoring.

Author: GitHub Copilot
Version: 2.0.0
"""

import os
import json
import csv
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import requests
from github import Github
from dotenv import load_dotenv


class VulnerabilityScanner:
    """
    Professional vulnerability scanner for GitHub Enterprise organizations.
    
    Features:
    - Complete Dependabot alert extraction
    - Intelligent CVSS score mapping
    - Comprehensive error handling
    - Rate limiting protection
    - Professional logging
    """
    
    # CVSS score defaults for missing scores
    SEVERITY_CVSS_MAPPING = {
        'critical': 9.0,
        'high': 7.0,
        'medium': 5.0,
        'low': 3.0
    }
    
    def __init__(self, github_token: str, base_url: str = None):
        """
        Initialize the vulnerability scanner.
        
        Args:
            github_token: GitHub personal access token
            base_url: GitHub base URL (auto-detected from environment)
        """
        # Get base URL from environment or use default
        if base_url is None:
            # Try GITHUB_URL first (for personal accounts), then GITHUB_ENTERPRISE_URL
            github_url = os.getenv('GITHUB_URL', '')
            if github_url and 'github.com' in github_url:
                # For public GitHub
                base_url = 'https://github.com'
            else:
                # For enterprise or API URL
                base_url = os.getenv('GITHUB_ENTERPRISE_URL', 'https://github.com')
        
        # Remove /api/v3 suffix if present and ensure proper format
        self.base_url = base_url.rstrip('/').replace('/api/v3', '')
        self.api_base = f"{self.base_url}/api/v3"
        self.github_token = github_token
        
        # Initialize GitHub client (use None for public GitHub.com)
        if 'github.com' in self.base_url and self.base_url == 'https://github.com':
            # For public GitHub, don't specify base_url
            self.github_client = Github(login_or_token=github_token)
        else:
            # For Enterprise GitHub
            self.github_client = Github(base_url=f"{self.base_url}/api/v3", login_or_token=github_token)
        
        # Configure session for API calls
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'Security-Vulnerability-Scanner/2.0'
        })
        
        self.stats = {
            'repositories_scanned': 0,
            'vulnerabilities_found': 0,
            'repositories_with_vulnerabilities': 0,
            'scan_errors': 0
        }
    
    def get_organization_repositories(self, org_name: str) -> List[str]:
        """
        Fetch all repository names from the organization or user account.
        
        Args:
            org_name: Organization name or username
            
        Returns:
            List of repository names
        """
        print(f"Fetching repositories from {org_name}...")
        
        try:
            # First try as organization
            try:
                org = self.github_client.get_organization(org_name)
                repos = []
                page = 1
                
                # Paginate through all repositories
                while True:
                    try:
                        repo_page = list(org.get_repos(type='all').get_page(page - 1))
                        if not repo_page:
                            break
                            
                        print(f"  Found {len(repo_page)} repositories on page {page}")
                        repos.extend([repo.name for repo in repo_page])
                        page += 1
                    except Exception as e:
                        if page == 1:
                            # If first page fails, try without pagination
                            all_repos = list(org.get_repos(type='all'))
                            repos.extend([repo.name for repo in all_repos])
                            print(f"  Found {len(all_repos)} repositories")
                        break
                
                print(f"Total repositories found: {len(repos)} (organization)")
                return repos
                
            except Exception as org_error:
                # If organization fails, try as user account
                print(f"  Not an organization, trying as user account...")
                user = self.github_client.get_user(org_name)
                repos = []
                
                # Get all user repositories
                all_repos = list(user.get_repos(type='all'))
                repos.extend([repo.name for repo in all_repos])
                print(f"  Found {len(repos)} repositories (user account)")
                
                print(f"Total repositories found: {len(repos)}")
                return repos
            
        except Exception as e:
            print(f"‚ùå Error fetching repositories: {e}")
            return []
    
    def get_repository_vulnerabilities(self, org_name: str, repo_name: str) -> List[Dict]:
        """
        Extract all Dependabot vulnerabilities from a repository.
        
        Args:
            org_name: Organization name
            repo_name: Repository name
            
        Returns:
            List of vulnerability dictionaries
        """
        url = f"{self.api_base}/repos/{org_name}/{repo_name}/dependabot/alerts"
        vulnerabilities = []
        page = 1
        repo_obj = None
        
        while True:
            try:
                response = self.session.get(url, params={'page': page, 'per_page': 100})
                
                if response.status_code == 403:
                    print(f"  No access to Dependabot alerts for {repo_name}")
                    break
                elif response.status_code == 404:
                    # Repository might be archived or private
                    try:
                        repo_obj = self.github_client.get_repo(f"{org_name}/{repo_name}")
                        if repo_obj.archived:
                            print(f"  Skipping archived repository: {repo_name}")
                        break
                    except:
                        break
                elif response.status_code != 200:
                    print(f"  ‚ö†Ô∏è  Error {response.status_code} for {repo_name}")
                    break
                
                alerts = response.json()
                if not alerts:
                    break
                
                # Get repository object for version extraction if not already obtained
                if repo_obj is None:
                    try:
                        repo_obj = self.github_client.get_repo(f"{org_name}/{repo_name}")
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è  Could not access repository {repo_name}: {e}")
                
                # Process each alert
                for alert in alerts:
                    vulnerability = self._process_vulnerability_alert(repo_name, alert)
                    if vulnerability:
                        # Extract current version from manifest file
                        if repo_obj:
                            current_version = self._extract_current_version(
                                repo_obj,
                                vulnerability.get('manifest_path'),
                                vulnerability.get('package_name'),
                                vulnerability.get('package_ecosystem')
                            )
                            vulnerability['current_version'] = current_version
                            
                            # Log version extraction for debugging
                            if current_version in ['File Not Found', 'Access Denied']:
                                print(f"    üìÅ {vulnerability.get('manifest_path')} ‚Üí {current_version}")
                            elif current_version not in ['Not Available', 'Workflow File', 'Non-Standard Manifest']:
                                print(f"    ‚úÖ {vulnerability.get('package_name')}: {current_version}")
                        
                        vulnerabilities.append(vulnerability)
                
                page += 1
                
            except Exception as e:
                print(f"  ‚ùå Error processing {repo_name}: {e}")
                self.stats['scan_errors'] += 1
                break
        
        return vulnerabilities
    
    def _process_vulnerability_alert(self, repo_name: str, alert: Dict) -> Optional[Dict]:
        """
        Process and normalize a Dependabot alert with intelligent CVSS scoring.
        
        Args:
            repo_name: Repository name
            alert: Raw alert data from GitHub API
            
        Returns:
            Processed vulnerability dictionary
        """
        try:
            security_advisory = alert.get('security_advisory', {})
            dependency = alert.get('dependency', {})
            security_vulnerability = alert.get('security_vulnerability', {})
            
            # Extract CVSS score with intelligent fallback
            cvss_score = security_advisory.get('cvss', {}).get('score')
            severity = security_advisory.get('severity', '').lower()
            
            # Apply intelligent CVSS mapping for missing scores
            if cvss_score is None or cvss_score == 0:
                cvss_score = self.SEVERITY_CVSS_MAPPING.get(severity, 0.0)
            
            return {
                # Repository info
                'repository': repo_name,
                'alert_number': alert.get('number'),
                'alert_state': alert.get('state'),
                'alert_url': alert.get('html_url'),
                
                # Detailed timing and status info
                'alert_created_at': alert.get('created_at'),
                'alert_updated_at': alert.get('updated_at'),
                'alert_dismissed_at': alert.get('dismissed_at'),
                'alert_fixed_at': alert.get('fixed_at'),
                'auto_dismissed_at': alert.get('auto_dismissed_at'),
                
                # Dismissal and resolution details
                'alert_dismissed_reason': alert.get('dismissed_reason'),
                'alert_dismissed_comment': alert.get('dismissed_comment'),
                'dismisser_login': alert.get('dismisser', {}).get('login') if alert.get('dismisser') else None,
                'dismisser_type': alert.get('dismisser', {}).get('type') if alert.get('dismisser') else None,
                
                # Resolution tracking
                'resolution_method': self._determine_resolution_method(alert),
                'days_to_resolution': self._calculate_resolution_days(alert),
                'current_status': self._get_detailed_status(alert),
                
                # Security advisory details
                'cve_id': security_advisory.get('cve_id'),
                'ghsa_id': security_advisory.get('ghsa_id'),
                'summary': security_advisory.get('summary'),
                'description': security_advisory.get('description'),
                'severity': security_advisory.get('severity'),
                'cvss_score': cvss_score,
                'cvss_vector': security_advisory.get('cvss', {}).get('vector_string'),
                'cwe_ids': ','.join([cwe.get('cwe_id', '') for cwe in security_advisory.get('cwes', [])]),
                
                # Publication and lifecycle info
                'published_at': security_advisory.get('published_at'),
                'updated_at': security_advisory.get('updated_at'),
                'withdrawn_at': security_advisory.get('withdrawn_at'),
                
                # Dependency details
                'package_ecosystem': dependency.get('package', {}).get('ecosystem'),
                'package_name': dependency.get('package', {}).get('name'),
                'manifest_path': dependency.get('manifest_path'),
                'scope': dependency.get('scope'),
                
                # Vulnerability specifics
                'vulnerable_version_range': security_vulnerability.get('vulnerable_version_range'),
                'first_patched_version': (
                    security_vulnerability.get('first_patched_version', {}).get('identifier')
                    if security_vulnerability.get('first_patched_version') else None
                ),
                
                # Additional metadata
                'alert_created_date': self._format_date(alert.get('created_at')),
                'alert_fixed_date': self._format_date(alert.get('fixed_at')),
                'alert_dismissed_date': self._format_date(alert.get('dismissed_at')),
                'vulnerability_age_days': self._calculate_age_days(alert.get('created_at')),
                
                # Current version from manifest file
                'current_version': 'Not Available',  # Will be populated during processing
            }
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error processing alert: {e}")
            return None
    
    def _extract_current_version(self, repo_obj, manifest_path: str, package_name: str, ecosystem: str) -> str:
        """
        Extract current version of a package from its manifest file.
        
        Args:
            repo_obj: GitHub repository object
            manifest_path: Path to the manifest file (may be URL-style path)
            package_name: Name of the package
            ecosystem: Package ecosystem (npm, pip, maven, etc.)
            
        Returns:
            Current version string or 'Not Available'
        """
        try:
            if not manifest_path or not package_name:
                return 'Not Available'
            
            # Clean up the manifest path - remove URL-style prefixes
            clean_path = manifest_path
            
            # Handle GitLab-style paths: "repo/blob/-/file.json" -> "file.json"
            if '/blob/-/' in clean_path:
                clean_path = clean_path.split('/blob/-/')[-1]
            
            # Handle GitHub-style paths: "repo/blob/main/file.json" -> "file.json" 
            elif '/blob/' in clean_path:
                parts = clean_path.split('/blob/')
                if len(parts) > 1:
                    # Get everything after the branch part
                    branch_and_file = parts[-1]
                    if '/' in branch_and_file:
                        clean_path = '/'.join(branch_and_file.split('/')[1:])
                    else:
                        clean_path = branch_and_file
            
            # Debug logging to see path transformation
            if clean_path != manifest_path:
                print(f"    üîß Path cleaned: {manifest_path} ‚Üí {clean_path}")
            
            # Skip workflow files and other non-manifest files
            if (clean_path.startswith('.github/workflows/') or 
                clean_path.endswith(('.yml', '.yaml')) or
                '/workflows/' in clean_path):
                return 'Workflow File'
            
            # Enhanced file type detection - be more permissive
            supported_files = {
                'npm': ['package.json', 'package-lock.json', 'yarn.lock'],
                'pip': ['requirements.txt', 'requirements.in', 'requirements-dev.txt', 'requirements-test.txt', 
                       'requirements-prod.txt', 'requirements.pip', 'requirements/base.txt',
                       'requirements/dev.txt', 'requirements/prod.txt', 'requirements/test.txt',
                       'pyproject.toml', 'setup.py', 'setup.cfg', 'Pipfile', 'Pipfile.lock',
                       'constraints.txt', 'pip-requirements.txt', 'poetry.lock', 'poetry.toml'],
                'maven': ['pom.xml', 'build.gradle', 'build.gradle.kts'],
                'rubygems': ['Gemfile', 'Gemfile.lock', '.gemspec'],
                'cargo': ['Cargo.toml', 'Cargo.lock'],
                'nuget': ['.csproj', '.fsproj', '.vbproj', 'packages.config', 'Directory.Build.props'],
                'composer': ['composer.json', 'composer.lock']
            }
            
            # Check if this is a supported file type for the ecosystem
            if ecosystem in supported_files:
                is_supported = any(
                    clean_path.endswith(ext) or 
                    ext in clean_path or 
                    clean_path.split('/')[-1] == ext
                    for ext in supported_files[ecosystem]
                )
                if not is_supported:
                    # Only log non-standard manifests for debugging, reduce noise for common cases
                    if not any(common in clean_path.lower() for common in ['poetry.lock', 'pipfile', 'pyproject']):
                        print(f"    üîç Non-standard manifest: {clean_path} for {ecosystem}")
                    return 'Non-Standard Manifest'
            
            # Get the manifest file content from the default branch (where Dependabot scans)
            try:
                content = repo_obj.get_contents(clean_path)
                file_content = content.decoded_content.decode('utf-8')
            except Exception as e:
                error_str = str(e).lower()
                if '404' in error_str or 'not found' in error_str:
                    # Try alternative file names in the same directory before giving up
                    alternative_result = self._try_alternative_manifest_files(repo_obj, clean_path, package_name, ecosystem)
                    if alternative_result != 'File Not Found':
                        return alternative_result
                    
                    # Only print file not found for non-standard paths to reduce noise
                    if not any(std_file in clean_path for std_file in ['requirements.txt', 'package.json', 'pom.xml']):
                        print(f"    üìÅ {clean_path} ‚Üí File Not Found")
                    return 'File Not Found'
                elif '403' in error_str or 'forbidden' in error_str:
                    return 'Access Denied'
                else:
                    return 'Access Error'
            
            # Parse based on ecosystem and file type with enhanced logic
            if ecosystem == 'pip':
                # Handle all types of Python requirement files
                if any(req_type in clean_path.lower() for req_type in [
                    'requirements', 'constraints', 'pipfile'
                ]):
                    result = self._parse_pip_version(file_content, package_name)
                    if result != 'Not Found in Requirements':
                        return result
                    # If not found, try alternative parsing methods
                    return self._try_alternative_pip_parsing(file_content, package_name, clean_path)
                elif 'pyproject.toml' in clean_path:
                    return self._parse_pyproject_version(file_content, package_name)
                elif 'poetry.lock' in clean_path:
                    return self._parse_poetry_lock_version(file_content, package_name)
                elif 'setup.py' in clean_path:
                    return self._parse_setup_py_version(file_content, package_name)
                else:
                    # Try generic pip parsing anyway
                    return self._parse_pip_version(file_content, package_name)
            elif ecosystem == 'npm':
                if 'package.json' in clean_path:
                    return self._parse_npm_version(file_content, package_name)
                elif 'package-lock.json' in clean_path:
                    return self._parse_npm_lock_version(file_content, package_name)
                elif 'yarn.lock' in clean_path:
                    return self._parse_yarn_lock_version(file_content, package_name)
            elif ecosystem == 'maven':
                if 'pom.xml' in clean_path:
                    return self._parse_maven_version(file_content, package_name)
                elif 'gradle' in clean_path:
                    return self._parse_gradle_version(file_content, package_name)
            elif ecosystem == 'rubygems':
                return self._parse_ruby_version(file_content, package_name)
            elif ecosystem == 'cargo':
                return self._parse_cargo_version(file_content, package_name)
            else:
                print(f"    üîç Ecosystem: {ecosystem}, File: {clean_path}")
                return 'Unsupported Format'
                
        except Exception as e:
            # Handle common GitHub API errors more gracefully
            error_msg = str(e)
            if '404' in error_msg:
                return 'File Not Found'
            elif '403' in error_msg:
                return 'Access Denied'
            elif 'rate limit' in error_msg.lower():
                return 'Rate Limited'
            else:
                return 'Parse Error'
    
    def _parse_npm_version(self, content: str, package_name: str) -> str:
        """Parse version from package.json with enhanced pattern matching."""
        import json
        try:
            data = json.loads(content)
            dependencies = data.get('dependencies', {})
            dev_dependencies = data.get('devDependencies', {})
            peer_dependencies = data.get('peerDependencies', {})
            
            # Check all dependency sections
            all_deps = {**dependencies, **dev_dependencies, **peer_dependencies}
            
            # Direct match first
            if package_name in all_deps:
                version = all_deps[package_name].lstrip('^~>=<')
                print(f"    ‚úÖ {package_name}: {version}")
                return version
            
            # Case-insensitive fallback
            for dep_name, version in all_deps.items():
                if dep_name.lower() == package_name.lower():
                    clean_version = version.lstrip('^~>=<')
                    print(f"    ‚úÖ {package_name}: {clean_version} (case-insensitive)")
                    return clean_version
            
            print(f"    ‚ùå {package_name}: Not Found in Dependencies")
            return 'Not Found in Dependencies'
        except Exception as e:
            print(f"    ‚ùå {package_name}: Parse Error - {e}")
            return 'Parse Error'
    
    def _parse_npm_lock_version(self, content: str, package_name: str) -> str:
        """Parse version from package-lock.json."""
        import json
        try:
            data = json.loads(content)
            
            # Check in packages section (npm v7+)
            packages = data.get('packages', {})
            for path, package_info in packages.items():
                if path.endswith(f'node_modules/{package_name}') or path == f'node_modules/{package_name}':
                    return package_info.get('version', 'Version Not Found')
            
            # Check in dependencies section (npm v6 and earlier)
            dependencies = data.get('dependencies', {})
            if package_name in dependencies:
                return dependencies[package_name].get('version', 'Version Not Found')
            
            return 'Not Found in Lock File'
        except Exception:
            return 'Parse Error'
    
    def _try_alternative_pip_parsing(self, content: str, package_name: str, file_path: str) -> str:
        """Try alternative parsing strategies for pip requirements."""
        try:
            print(f"    üîÑ Trying alternative parsing for {package_name} in {file_path}")
            
            # Strategy 1: Look for any line mentioning the package (very permissive)
            for line in content.split('\n'):
                line_lower = line.strip().lower()
                if not line_lower or line_lower.startswith('#'):
                    continue
                
                # Check if package name appears anywhere in the line
                if package_name.lower() in line_lower:
                    # Try to extract any version number from the line
                    import re
                    version_patterns = [
                        r'(\d+\.\d+\.\d+[a-zA-Z0-9\.\-\+]*)',  # Standard version
                        r'(\d+\.\d+[a-zA-Z0-9\.\-\+]*)',      # Major.minor
                        r'(\d+[a-zA-Z0-9\.\-\+]*)',           # Any number start
                    ]
                    
                    for pattern in version_patterns:
                        matches = re.findall(pattern, line)
                        if matches:
                            # Take the first reasonable version found
                            version = matches[0]
                            print(f"    ‚úÖ {package_name}: {version} (alternative parsing)")
                            return version
            
            # Strategy 2: Check if it's a constraint file format
            if 'constraint' in file_path.lower():
                return self._parse_constraints_file(content, package_name)
            
            return 'Not Found in Requirements'
            
        except Exception as e:
            print(f"    ‚ùå Alternative parsing failed for {package_name}: {e}")
            return 'Parse Error'
    
    def _parse_constraints_file(self, content: str, package_name: str) -> str:
        """Parse constraints.txt files which may have different format."""
        import re
        try:
            for line in content.split('\n'):
                line = line.strip()
                if package_name.lower() in line.lower():
                    # Constraints might use different operators
                    match = re.search(rf'{re.escape(package_name)}\s*([><=!~]*)\s*([0-9][0-9a-zA-Z\.\-\+]*)', line, re.IGNORECASE)
                    if match and match.group(2):
                        return match.group(2)
            return 'Not Found in Constraints'
        except Exception:
            return 'Parse Error'
    
    def _try_alternative_manifest_files(self, repo_obj, original_path: str, package_name: str, ecosystem: str) -> str:
        """Try to find alternative manifest files when the original path doesn't exist."""
        try:
            # Get directory path from original file
            path_parts = original_path.split('/')
            if len(path_parts) < 2:
                return 'File Not Found'
            
            directory = '/'.join(path_parts[:-1])  # Remove filename
            
            # Define alternative file names for each ecosystem
            alternatives = {
                'pip': ['requirements.txt', 'pyproject.toml', 'setup.py', 'Pipfile', 'poetry.lock'],
                'npm': ['package.json', 'package-lock.json', 'yarn.lock'],
                'maven': ['pom.xml', 'build.gradle'],
                'rubygems': ['Gemfile', 'Gemfile.lock'],
                'cargo': ['Cargo.toml', 'Cargo.lock']
            }
            
            if ecosystem not in alternatives:
                return 'File Not Found'
            
            # Try each alternative file in the same directory
            for alt_file in alternatives[ecosystem]:
                alt_path = f"{directory}/{alt_file}"
                try:
                    content = repo_obj.get_contents(alt_path)
                    file_content = content.decoded_content.decode('utf-8')
                    
                    # Parse using the appropriate method
                    if ecosystem == 'pip':
                        if 'requirements' in alt_file or 'Pipfile' in alt_file:
                            result = self._parse_pip_version(file_content, package_name)
                        elif 'pyproject.toml' in alt_file:
                            result = self._parse_pyproject_version(file_content, package_name)
                        elif 'poetry.lock' in alt_file:
                            result = self._parse_poetry_lock_version(file_content, package_name)
                        elif 'setup.py' in alt_file:
                            result = self._parse_setup_py_version(file_content, package_name)
                        else:
                            continue
                            
                        if result not in ['Not Found in Requirements', 'Parse Error']:
                            print(f"    üîÑ Found in alternative file: {alt_path}")
                            return result
                            
                except Exception:
                    continue  # Try next alternative
            
            return 'File Not Found'
            
        except Exception:
            return 'File Not Found'
    
    def _parse_poetry_lock_version(self, content: str, package_name: str) -> str:
        """Parse version from poetry.lock file."""
        try:
            # Poetry lock files have a specific TOML format
            # Look for [[package]] sections with name and version
            
            lines = content.split('\n')
            current_package = None
            current_version = None
            
            for line in lines:
                line = line.strip()
                
                # Look for package name in [[package]] sections
                if line.startswith('name = '):
                    # Extract package name: name = "package-name"
                    import re
                    name_match = re.search(r'name\s*=\s*["\']([^"\']+)["\']', line)
                    if name_match:
                        current_package = name_match.group(1)
                
                # Look for version in the same package section
                elif line.startswith('version = ') and current_package:
                    # Extract version: version = "1.2.3"
                    import re
                    version_match = re.search(r'version\s*=\s*["\']([^"\']+)["\']', line)
                    if version_match:
                        current_version = version_match.group(1)
                        
                        # Check if this is the package we're looking for
                        if self._package_names_match(current_package, package_name):
                            print(f"    ‚úÖ {package_name}: {current_version} (from poetry.lock)")
                            return current_version
                
                # Reset when we hit a new section or empty line
                elif line.startswith('[[') or line == '':
                    current_package = None
                    current_version = None
            
            print(f"    ‚ùå {package_name}: Not Found in Poetry Lock")
            return 'Not Found in Poetry Lock'
            
        except Exception as e:
            print(f"    ‚ùå {package_name}: Poetry Parse Error - {e}")
            return 'Parse Error'
    
    def _package_names_match(self, manifest_name: str, search_name: str) -> bool:
        """Check if package names match considering common variations."""
        if not manifest_name or not search_name:
            return False
            
        # Direct match
        if manifest_name == search_name:
            return True
            
        # Case insensitive match
        if manifest_name.lower() == search_name.lower():
            return True
            
        # Handle underscore/hyphen variations
        normalized_manifest = manifest_name.lower().replace('-', '_')
        normalized_search = search_name.lower().replace('-', '_')
        
        return normalized_manifest == normalized_search
    
    def _parse_pip_version(self, content: str, package_name: str) -> str:
        """Parse version from requirements.txt with enhanced pattern matching."""
        import re
        try:
            # Create multiple variations of the package name to handle different formats
            name_variations = [
                package_name,                           # Original name
                package_name.lower(),                   # lowercase
                package_name.upper(),                   # uppercase  
                package_name.capitalize(),              # Capitalized
                package_name.replace('-', '_'),         # hyphens to underscores
                package_name.replace('_', '-'),         # underscores to hyphens
                package_name.lower().replace('-', '_'), # lowercase with underscores
                package_name.lower().replace('_', '-'), # lowercase with hyphens
            ]
            
            # Remove duplicates while preserving order
            name_variations = list(dict.fromkeys(name_variations))
            
            for variation in name_variations:
                # Multiple patterns to catch different requirement formats
                patterns = [
                    # Standard formats: package==1.2.3, package>=1.2.3, etc.
                    rf'^{re.escape(variation)}\s*([><=!~]+)\s*([0-9][0-9a-zA-Z\.\-\+]+)',
                    # Package with extras: package[extra]==1.2.3
                    rf'^{re.escape(variation)}\[.*?\]\s*([><=!~]+)\s*([0-9][0-9a-zA-Z\.\-\+]+)',
                    # Whitespace variations
                    rf'^\s*{re.escape(variation)}\s*([><=!~]+)\s*([0-9][0-9a-zA-Z\.\-\+]+)',
                    # With comments inline
                    rf'^{re.escape(variation)}\s*([><=!~]+)\s*([0-9][0-9a-zA-Z\.\-\+]+)\s*#.*',
                ]
                
                for line in content.split('\n'):
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                        
                    # Remove inline comments for cleaner parsing
                    clean_line = line.split('#')[0].strip()
                    
                    for pattern in patterns:
                        match = re.match(pattern, clean_line, re.IGNORECASE)
                        if match:
                            version = match.group(2).strip()
                            print(f"    ‚úÖ {package_name}: {version} (matched as '{variation}')")
                            return version
            
            # Last resort: fuzzy search for any line containing the package name and version pattern
            for line in content.split('\n'):
                clean_line = line.strip().lower()
                if not clean_line or clean_line.startswith('#'):
                    continue
                    
                # Check if line contains package name and has version operators
                if any(name.lower() in clean_line for name in name_variations):
                    if any(op in clean_line for op in ['==', '>=', '<=', '~=', '!=', '>']):
                        # Try to extract version from anywhere in the line
                        version_match = re.search(r'([><=!~]+)\s*([0-9][0-9a-zA-Z\.\-\+]+)', clean_line)
                        if version_match:
                            version = version_match.group(2)
                            print(f"    ‚úÖ {package_name}: {version} (fuzzy match)")
                            return version
            
            print(f"    ‚ùå {package_name}: Not Found in Requirements")
            return 'Not Found in Requirements'
            
        except Exception as e:
            print(f"    ‚ùå {package_name}: Parse Error - {e}")
            return 'Parse Error'
    
    def _parse_maven_version(self, content: str, package_name: str) -> str:
        """Parse version from pom.xml."""
        import re
        try:
            # Look for dependency with artifact ID matching package name
            artifact_pattern = rf'<artifactId>{re.escape(package_name)}</artifactId>'
            version_pattern = r'<version>(.*?)</version>'
            
            # Find the dependency block
            dependency_blocks = re.findall(r'<dependency>.*?</dependency>', content, re.DOTALL)
            
            for block in dependency_blocks:
                if re.search(artifact_pattern, block):
                    version_match = re.search(version_pattern, block)
                    if version_match:
                        return version_match.group(1)
            
            return 'Not Found in POM'
        except Exception:
            return 'Parse Error'
    
    def _parse_ruby_version(self, content: str, package_name: str) -> str:
        """Parse version from Gemfile or Gemfile.lock."""
        import re
        try:
            # Look for gem 'package_name', 'version' patterns
            pattern = rf"gem\s+['\"]?{re.escape(package_name)}['\"]?\s*,\s*['\"]([^'\"]+)['\"]"
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).lstrip('~>')
            
            # Also try Gemfile.lock format
            lock_pattern = rf'{re.escape(package_name)}\s+\(([^)]+)\)'
            lock_match = re.search(lock_pattern, content, re.IGNORECASE)
            if lock_match:
                return lock_match.group(1)
                
            return 'Not Found in Gemfile'
        except Exception:
            return 'Parse Error'
    
    def _parse_cargo_version(self, content: str, package_name: str) -> str:
        """Parse version from Cargo.toml."""
        import re
        try:
            # Look for package_name = "version" patterns
            pattern = rf'{re.escape(package_name)}\s*=\s*["\']([^"\']+)["\']'
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).lstrip('^~>=<')
            
            return 'Not Found in Cargo.toml'
        except Exception:
            return 'Parse Error'
    
    def _parse_pyproject_version(self, content: str, package_name: str) -> str:
        """Parse version from pyproject.toml (supports both Poetry and PEP 621 formats)."""
        import re
        try:
            # Strategy 1: Look for Poetry-style dependencies [tool.poetry.dependencies]
            poetry_section = False
            dev_section = False
            
            for line in content.split('\n'):
                line = line.strip()
                
                # Track which section we're in
                if '[tool.poetry.dependencies]' in line:
                    poetry_section = True
                    dev_section = False
                    continue
                elif '[tool.poetry.dev-dependencies]' in line or '[tool.poetry.group.dev.dependencies]' in line:
                    poetry_section = False
                    dev_section = True
                    continue
                elif line.startswith('[') and line.endswith(']'):
                    poetry_section = False
                    dev_section = False
                    continue
                
                # Look for package in current section
                if (poetry_section or dev_section) and self._package_names_match(line.split('=')[0].strip().strip('"\''), package_name):
                    # Parse Poetry dependency format: package = "^1.2.3" or package = {version = "^1.2.3"}
                    if '=' in line:
                        version_part = line.split('=', 1)[1].strip()
                        
                        # Simple version: package = "^1.2.3"
                        simple_match = re.search(r'["\']([^"\']+)["\']', version_part)
                        if simple_match:
                            version = simple_match.group(1).lstrip('^~>=<')
                            print(f"    ‚úÖ {package_name}: {version} (from pyproject.toml)")
                            return version
                        
                        # Complex version: package = {version = "^1.2.3", optional = true}
                        complex_match = re.search(r'version\s*=\s*["\']([^"\']+)["\']', version_part)
                        if complex_match:
                            version = complex_match.group(1).lstrip('^~>=<')
                            print(f"    ‚úÖ {package_name}: {version} (from pyproject.toml)")
                            return version
            
            # Strategy 2: Look for PEP 621 style dependencies
            pep621_match = re.search(rf'{re.escape(package_name)}\s*([><=!~]+)\s*([0-9][0-9a-zA-Z\.\-\+]+)', content, re.IGNORECASE)
            if pep621_match:
                version = pep621_match.group(2)
                print(f"    ‚úÖ {package_name}: {version} (from pyproject.toml PEP621)")
                return version
            
            return 'Not Found in pyproject.toml'
        except Exception as e:
            print(f"    ‚ùå {package_name}: pyproject.toml Parse Error - {e}")
            return 'Parse Error'
    
    def _parse_setup_py_version(self, content: str, package_name: str) -> str:
        """Parse version from setup.py."""
        import re
        try:
            # Look for package in install_requires or requires
            pattern = rf'["\'{re.escape(package_name)}[><=!~\[]+([0-9][0-9a-zA-Z\.\-\+]+)'
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1)
            return 'Not Found in setup.py'
        except Exception:
            return 'Parse Error'
    
    def _parse_yarn_lock_version(self, content: str, package_name: str) -> str:
        """Parse version from yarn.lock."""
        import re
        try:
            # Look for package@version pattern
            pattern = rf'{re.escape(package_name)}@[^:]+:\s*version\s+"([^"]+)"'
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1)
            return 'Not Found in Yarn Lock'
        except Exception:
            return 'Parse Error'
    
    def _parse_gradle_version(self, content: str, package_name: str) -> str:
        """Parse version from build.gradle."""
        import re
        try:
            # Look for dependency declaration
            patterns = [
                rf"['\"]?{re.escape(package_name)}['\"]?\s*:\s*['\"]([^'\"]+)['\"]",
                rf"implementation\s+['\"].*?{re.escape(package_name)}:([^'\"]+)['\"]",
                rf"compile\s+['\"].*?{re.escape(package_name)}:([^'\"]+)['\"]"
            ]
            for pattern in patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    return match.group(1)
            return 'Not Found in Gradle'
        except Exception:
            return 'Parse Error'

    def _determine_resolution_method(self, alert: Dict) -> str:
        """Determine how the vulnerability was resolved."""
        if alert.get('fixed_at'):
            return 'Fixed'
        elif alert.get('dismissed_at'):
            reason = alert.get('dismissed_reason', '').lower()
            if 'false positive' in reason:
                return 'Dismissed - False Positive'
            elif 'won\'t fix' in reason or 'wont fix' in reason:
                return 'Dismissed - Won\'t Fix'
            elif 'tolerable risk' in reason:
                return 'Dismissed - Tolerable Risk'
            elif 'used in tests' in reason:
                return 'Dismissed - Used in Tests'
            else:
                return f'Dismissed - {reason.title()}'
        elif alert.get('state') == 'open':
            return 'Open'
        else:
            return 'Unknown'
    
    def _calculate_resolution_days(self, alert: Dict) -> Optional[int]:
        """Calculate days from creation to resolution."""
        from datetime import datetime
        
        created_at = alert.get('created_at')
        resolution_date = alert.get('fixed_at') or alert.get('dismissed_at')
        
        if not created_at or not resolution_date:
            return None
        
        try:
            created = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            resolved = datetime.fromisoformat(resolution_date.replace('Z', '+00:00'))
            return (resolved - created).days
        except:
            return None
    
    def _get_detailed_status(self, alert: Dict) -> str:
        """Get detailed status description."""
        state = alert.get('state', 'unknown').upper()
        
        if state == 'FIXED':
            return 'RESOLVED - Fixed'
        elif state == 'DISMISSED':
            reason = alert.get('dismissed_reason', 'Unknown')
            return f'RESOLVED - Dismissed ({reason})'
        elif state == 'OPEN':
            severity = alert.get('security_advisory', {}).get('severity', 'unknown').upper()
            age_days = self._calculate_age_days(alert.get('created_at'))
            if age_days:
                return f'OPEN - {severity} ({age_days} days old)'
            else:
                return f'OPEN - {severity}'
        else:
            return f'{state}'
    
    def _format_date(self, date_string: Optional[str]) -> Optional[str]:
        """Format ISO date string to readable format."""
        if not date_string:
            return None
        
        try:
            from datetime import datetime
            dt = datetime.fromisoformat(date_string.replace('Z', '+00:00'))
            return dt.strftime('%Y-%m-%d %H:%M:%S UTC')
        except:
            return date_string
    
    def _calculate_age_days(self, created_at: Optional[str]) -> Optional[int]:
        """Calculate age of vulnerability in days."""
        if not created_at:
            return None
        
        try:
            from datetime import datetime
            created = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            now = datetime.now(created.tzinfo)
            return (now - created).days
        except:
            return None
    
    def scan_organization(self, org_name: str) -> List[Dict]:
        """
        Scan entire organization or user account for vulnerabilities.
        
        Args:
            org_name: Organization name or username to scan
            
        Returns:
            List of all vulnerabilities found
        """
        print(f"üîç Starting vulnerability scan for {org_name}")
        print("=" * 70)
        
        # Get all repositories
        repositories = self.get_organization_repositories(org_name)
        if not repositories:
            print("‚ùå No repositories found or access denied")
            return []
        
        all_vulnerabilities = []
        
        # Scan each repository
        for i, repo_name in enumerate(repositories, 1):
            print(f"[{i}/{len(repositories)}] Processing {repo_name}...")
            self.stats['repositories_scanned'] += 1
            
            vulnerabilities = self.get_repository_vulnerabilities(org_name, repo_name)
            
            if vulnerabilities:
                print(f"  Found {len(vulnerabilities)} Dependabot alerts")
                all_vulnerabilities.extend(vulnerabilities)
                self.stats['repositories_with_vulnerabilities'] += 1
                self.stats['vulnerabilities_found'] += len(vulnerabilities)
            else:
                print(f"  No Dependabot alerts found")
        
        print(f"‚úÖ Vulnerability scan completed: {len(all_vulnerabilities)} vulnerabilities found")
        return all_vulnerabilities
    
    def save_results(self, vulnerabilities: List[Dict], timestamp: str) -> Tuple[str, str]:
        """
        Save scan results to JSON and CSV files.
        
        Args:
            vulnerabilities: List of vulnerability data
            timestamp: Timestamp string for file naming
            
        Returns:
            Tuple of (json_filename, csv_filename)
        """
        # Create filenames
        json_filename = f"temp_vulnerabilities_{timestamp}.json"
        csv_filename = f"temp_vulnerabilities_{timestamp}.csv"
        
        # Save JSON
        with open(json_filename, 'w') as f:
            json.dump(vulnerabilities, f, indent=2, default=str)
        
        # Save CSV
        if vulnerabilities:
            with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=vulnerabilities[0].keys())
                writer.writeheader()
                writer.writerows(vulnerabilities)
        
        return json_filename, csv_filename
    
    def print_statistics(self):
        """Print scan statistics."""
        print(f"\nüìä Scan Statistics:")
        print(f"   Repositories scanned: {self.stats['repositories_scanned']}")
        print(f"   Repositories with vulnerabilities: {self.stats['repositories_with_vulnerabilities']}")
        print(f"   Total vulnerabilities found: {self.stats['vulnerabilities_found']}")
        print(f"   Scan errors: {self.stats['scan_errors']}")


def main():
    """Main execution function."""
    # Load environment variables
    load_dotenv()
    
    github_token = os.getenv('GITHUB_TOKEN')
    if not github_token:
        print("‚ùå Error: GITHUB_TOKEN environment variable not set")
        print("Please set your GitHub Enterprise token in the .env file")
        return
    
    # Initialize scanner
    scanner = VulnerabilityScanner(github_token)
    
    # Example usage - replace with your organization
    org_name = os.getenv('GITHUB_ORG', 'your-organization')
    vulnerabilities = scanner.scan_organization(org_name)
    
    if vulnerabilities:
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file, csv_file = scanner.save_results(vulnerabilities, timestamp)
        
        print(f"\nüìÅ Results saved:")
        print(f"   JSON: {json_file}")
        print(f"   CSV: {csv_file}")
    
    # Print statistics
    scanner.print_statistics()


if __name__ == "__main__":
    main()